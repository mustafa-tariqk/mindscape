{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run these in terminal, or do it here (may require sudo password)\n",
    "# ! pip install langchain-openai langchain langchain_text_splitters tiktoken playwright beautifulsoup4 \n",
    "# ! playwright install-deps\n",
    "# ! playwright install\n",
    "\n",
    "# Set env var OPENAI_API_KEY or load from a .env file:\n",
    "import dotenv\n",
    "dotenv.load_dotenv('../../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load HTML\n",
    "The next code block resolves some async issues as described in https://github.com/langchain-ai/langchain/issues/9014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_extraction_chain\n",
    "\n",
    "category_schema = { # Multiple per page (should only appear in first split)\n",
    "    \"properties\": {\n",
    "        \"Dose\": {\"type\": \"string\"},\n",
    "        \"Method\": {\"type\": \"string\"},\n",
    "        \"Substance\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\"Substance\", \"Dose\", \"Method\"],\n",
    "}\n",
    "\n",
    "prop_schema = { # One per page (only in first split)\n",
    "    \"properties\": {\n",
    "        \"Author\": {\"type\": \"string\"},\n",
    "        \"Body Weight\": {\"type\": \"string\"},\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def extract(content: str, schema: dict):\n",
    "    return create_extraction_chain(schema=schema, llm=llm).invoke(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "\n",
    "# Map\n",
    "map_template = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please identify what the author is experiencing. Do not mention the substance name in your response. \n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "# Reduce, avoids clustering because of the substance name\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "{docs}\n",
    "Imagine you are retelling the story to someone in an elevator and want to hide the fact that the author is using psychedelics, you only have 30 seconds. \n",
    "Helpful Answer:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "# Run chain\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
    ")\n",
    "\n",
    "# Combines and iteratively reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000,\n",
    ")   \n",
    "\n",
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import AsyncChromiumLoader\n",
    "from langchain_community.document_transformers import BeautifulSoupTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def scrape_erowid(sub_id, prop_schema, category_schema):\n",
    "    result = {}\n",
    "    urls = [\"https://www.erowid.org/experiences/exp.php?ID=\" + str(sub_id)]\n",
    "    loader = AsyncChromiumLoader(urls)\n",
    "    docs = loader.load()\n",
    "    bs_transformer = BeautifulSoupTransformer()\n",
    "    docs_transformed = bs_transformer.transform_documents(\n",
    "        docs, unwanted_tags=[\"script\", \"style\", \"exclude\"], tags_to_extract=[\"div\", \"table\"], \n",
    "    )\n",
    "    \n",
    "    print(\"Extracting ID: \" + str(sub_id))\n",
    "\n",
    "    # Split into chunks of 2000\n",
    "    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=2000, chunk_overlap=0\n",
    "    )\n",
    "    splits = splitter.split_documents(docs_transformed)\n",
    "\n",
    "    if not splits or len(splits) == 0:\n",
    "        return None\n",
    "\n",
    "    # Process the categories (first split as it's at the top)\n",
    "    result[\"substances\"] = extract(schema=category_schema, content=splits[0].page_content)[\"text\"]\n",
    "    \n",
    "    # Process props (first split)\n",
    "    result[\"meta\"] = extract(schema=prop_schema, content=splits[0].page_content)[\"text\"]\n",
    "\n",
    "    # Map-reduce text to ensure token count isnt exceeded\n",
    "    result[\"experience\"] = map_reduce_chain.invoke(splits)[\"output_text\"]\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ID: 300\n",
      "Extracting ID: 301\n",
      "Extracting ID: 302\n",
      "Extracting ID: 303\n",
      "Extracting ID: 304\n",
      "Extracting ID: 305\n",
      "Extracting ID: 306\n",
      "Extracting ID: 307\n",
      "Extracting ID: 308\n",
      "Extracting ID: 309\n",
      "Extracting ID: 310\n",
      "Extracting ID: 311\n",
      "Extracting ID: 312\n",
      "Extracting ID: 313\n",
      "Extracting ID: 314\n",
      "Extracting ID: 315\n",
      "Extracting ID: 316\n",
      "Extracting ID: 317\n",
      "Extracting ID: 318\n",
      "Extracting ID: 319\n",
      "Extracting ID: 320\n",
      "Extracting ID: 321\n",
      "Extracting ID: 322\n",
      "Extracting ID: 323\n",
      "Extracting ID: 324\n",
      "Extracting ID: 325\n",
      "Extracting ID: 326\n",
      "Extracting ID: 327\n",
      "Extracting ID: 328\n",
      "Extracting ID: 329\n",
      "Extracting ID: 330\n",
      "Extracting ID: 331\n",
      "Extracting ID: 332\n",
      "Extracting ID: 333\n",
      "Extracting ID: 334\n",
      "Extracting ID: 335\n",
      "Extracting ID: 336\n",
      "Extracting ID: 337\n",
      "Extracting ID: 338\n",
      "Extracting ID: 339\n",
      "Extracting ID: 340\n",
      "Extracting ID: 341\n",
      "Extracting ID: 342\n",
      "Extracting ID: 343\n",
      "Extracting ID: 344\n",
      "Extracting ID: 345\n",
      "Extracting ID: 346\n",
      "Extracting ID: 347\n",
      "Extracting ID: 348\n",
      "Extracting ID: 349\n",
      "Extracting ID: 350\n",
      "Extracting ID: 351\n",
      "Extracting ID: 352\n",
      "Extracting ID: 353\n",
      "Extracting ID: 354\n",
      "Extracting ID: 355\n",
      "Extracting ID: 356\n",
      "Extracting ID: 357\n",
      "Extracting ID: 358\n",
      "Extracting ID: 359\n",
      "Extracting ID: 360\n",
      "Extracting ID: 361\n",
      "Extracting ID: 362\n",
      "Extracting ID: 363\n",
      "Extracting ID: 364\n",
      "Extracting ID: 365\n",
      "Extracting ID: 366\n",
      "Extracting ID: 367\n",
      "Extracting ID: 368\n",
      "Extracting ID: 369\n",
      "Extracting ID: 370\n",
      "Extracting ID: 371\n",
      "Extracting ID: 372\n",
      "Extracting ID: 373\n",
      "Extracting ID: 374\n",
      "Extracting ID: 375\n",
      "Extracting ID: 376\n",
      "Extracting ID: 377\n",
      "Extracting ID: 378\n",
      "Extracting ID: 379\n",
      "Extracting ID: 380\n",
      "Extracting ID: 381\n",
      "Extracting ID: 382\n",
      "Extracting ID: 383\n",
      "Extracting ID: 384\n",
      "Extracting ID: 385\n",
      "Extracting ID: 386\n",
      "Extracting ID: 387\n",
      "Extracting ID: 388\n",
      "Extracting ID: 389\n",
      "Extracting ID: 390\n",
      "Extracting ID: 391\n",
      "Extracting ID: 392\n",
      "Extracting ID: 393\n",
      "Extracting ID: 394\n",
      "Extracting ID: 395\n",
      "Extracting ID: 396\n",
      "Extracting ID: 397\n",
      "Extracting ID: 398\n",
      "Extracting ID: 399\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for sub_id in range(300, 400):\n",
    "    extracted_data = scrape_erowid(sub_id=sub_id, prop_schema=prop_schema, category_schema=category_schema)\n",
    "\n",
    "    if not extracted_data: continue # Some were removed\n",
    "\n",
    "    out_file = open(\"data/erowid/\" + str(sub_id) +\".json\", \"w\") \n",
    "  \n",
    "    json.dump(extracted_data, out_file, indent = 6) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7bc78d97afecc1842c289e6f45421d0895687e14e61eaa14fe580e037547793a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
